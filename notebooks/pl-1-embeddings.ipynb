{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Help Python find our packages\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from embeddings.randomwalk_embedder import *\n",
    "\n",
    "# Randomness\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./mock_metapaths.txt\"\n",
    "data = json.load(open(path, \"r\", encoding=\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = [len(path) for path in data.keys()]\n",
    "np.min(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_meta_paths(json, min_size = 5, seperator = \" | \"):\n",
    "        walk_list = []\n",
    "        available_nodes = set()\n",
    "        for meta_paths in json.keys():\n",
    "            node_ids = [int(id) for id in meta_paths.split(seperator)]\n",
    "            if (len(node_ids) < min_size):\n",
    "                continue\n",
    "            walk_list.append(node_ids)\n",
    "            available_nodes |= set(node_ids)\n",
    "        return walk_list, available_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_walk_list, pre_available_nodes = parse_meta_paths(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_nodes = range(len(pre_available_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_mapping = dict(zip(pre_available_nodes, available_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_list = []\n",
    "for mp in pre_walk_list:\n",
    "    path = []\n",
    "    for n in mp:\n",
    "        path.append(id_mapping[n])\n",
    "    walk_list.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  2.672363758087158\n",
      "Nearest to 2: 10, 4, 1, 6, 12, 8, 11, 0,\n",
      "Nearest to 3: 9, 1, 5, 10, 6, 12, 0, 7,\n",
      "Nearest to 8: 0, 7, 4, 11, 2, 5, 10, 9,\n",
      "Nearest to 12: 6, 1, 11, 10, 2, 5, 4, 7,\n",
      "Nearest to 1: 6, 3, 10, 12, 2, 9, 11, 5,\n",
      "NCE method took 0.432715 seconds to run 100 iterations\n"
     ]
    }
   ],
   "source": [
    "batch_generator = ShortWalkBatchGenerator(walk_list, available_nodes)\n",
    "embedded_nodes_size = len(batch_generator.available_nodes)\n",
    "\n",
    "batch_size = 128\n",
    "embedding_vector_size = 5  # Dimension of the embedding vector.\n",
    "num_skips = 2         # How many times to reuse a walk to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 5     # Random set of words to evaluate similarity on.\n",
    "valid_examples = random.sample(batch_generator.available_nodes, valid_size)\n",
    "num_sampled = 10    # Number of negative examples to sample. (relevant for NCE loss)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Look up embeddings for inputs.\n",
    "  embeddings = tf.Variable(\n",
    "      tf.random_uniform([embedded_nodes_size, embedding_vector_size], -1.0, 1.0))\n",
    "  embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "  # Construct the variables for the softmax\n",
    "  weights = tf.Variable(\n",
    "      tf.truncated_normal([embedding_vector_size, embedded_nodes_size],\n",
    "                          stddev=1.0 / math.sqrt(embedding_vector_size)))\n",
    "  biases = tf.Variable(tf.zeros([embedded_nodes_size]))\n",
    "  hidden_out = tf.transpose(tf.matmul(tf.transpose(weights), tf.transpose(embed))) + biases\n",
    "\n",
    "  # convert train_context to a one-hot format\n",
    "  train_one_hot = tf.one_hot(train_context, embedded_nodes_size)\n",
    "\n",
    "  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, labels=train_one_hot))\n",
    "\n",
    "  # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "  # Add variable initializer.\n",
    "  init = tf.global_variables_initializer()\n",
    "  # Finally, create out saver.\n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "def run(graph, num_steps):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      # We must initialize all variables before we use them.\n",
    "      init.run()\n",
    "      print('Initialized')\n",
    "\n",
    "      average_loss = 0\n",
    "      for step in range(num_steps):\n",
    "        batch_inputs, batch_context = batch_generator.generate_batch(batch_size, num_skips)\n",
    "#         print(batch_inputs)\n",
    "#         print(batch_context)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_context: batch_context}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, cross_entropy], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "          if step > 0:\n",
    "            average_loss /= 2000\n",
    "          # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "          print('Average loss at step ', step, ': ', average_loss)\n",
    "          average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "          sim = similarity.eval()\n",
    "          saver.save(session, \"./embedding.chkpt\", global_step=step)\n",
    "          index = 0\n",
    "          for valid_word in valid_examples:\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            nearest = (-sim[index, :]).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "              close_word = nearest[k]\n",
    "              log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "            index += 1\n",
    "      return normalized_embeddings.eval()\n",
    "\n",
    "# num_steps = 100\n",
    "# softmax_start_time = dt.datetime.now()\n",
    "# run(graph, num_steps=num_steps)\n",
    "# softmax_end_time = dt.datetime.now()\n",
    "# print(\"Softmax method took {} minutes to run 100 iterations\".format((softmax_end_time-softmax_start_time).total_seconds()))\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([embedded_nodes_size, embedding_vector_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_vector_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([embedded_nodes_size]))\n",
    "\n",
    "    nce_loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_context,\n",
    "                       inputs=embed,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=embedded_nodes_size))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(nce_loss)\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "num_steps = 50000\n",
    "nce_start_time = dt.datetime.now()\n",
    "embedding = run(graph, num_steps)\n",
    "nce_end_time = dt.datetime.now()\n",
    "print(\"NCE method took {} seconds to run 100 iterations\".format((nce_end_time-nce_start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.42824027,  0.47200668,  0.29335976, -0.49752355, -0.5101276 ],\n",
       "       [-0.3904592 ,  0.23337731,  0.36747465, -0.17365555, -0.79239047],\n",
       "       [-0.43053728, -0.13237327,  0.6927135 , -0.31631583, -0.46605518],\n",
       "       [-0.10324816,  0.17196442,  0.8304917 , -0.0554521 , -0.51669794],\n",
       "       [-0.3717345 , -0.04965382,  0.8213299 , -0.2679959 , -0.33607048],\n",
       "       [-0.3914409 ,  0.2498257 ,  0.683063  ,  0.1386725 , -0.54640275],\n",
       "       [-0.8267579 ,  0.34916762,  0.23643248,  0.06228579,  0.3671149 ],\n",
       "       [-0.69211215,  0.61120576,  0.19822869,  0.13105878,  0.30155814],\n",
       "       [-0.5232188 , -0.18998626,  0.5434246 ,  0.46480379, -0.42284077],\n",
       "       [ 0.22923012,  0.6550616 ,  0.61426634, -0.364336  , -0.09101606],\n",
       "       [-0.47140205,  0.5522466 ,  0.4377025 , -0.52546996,  0.0714256 ],\n",
       "       [-0.50748545,  0.78297555,  0.20342244,  0.24139987,  0.17249148],\n",
       "       [ 0.410743  ,  0.4041864 ,  0.04118409,  0.65856653, -0.48220062]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
